{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 1\n",
    "### 06/06/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first day of research was interesting as I was introduced to the NAO robots in terms of functionality, mobility, hardware, and software manipulation. The NAO robots have Dynamixel servos at every joint which give the robot 25 degrees of freedom. Each servo allows the robot to move independently and provide it with \"stiffness\". In addition to its servos, the robot has controllers, two sensors at its feet, ultrasonic sensors in its chest area to measure distance, two cameras (one at its forehead and one at its mouth), and an on-board controller. The ultrasonic sensors emit a sound which then bounce back into whatever object is in front of the robot, then the robot takes this information to measure the distance between itself and the object. The servos (or motors) can be a little expensive, ranging from 10 to 50 dollars or more depending on whether they are made of metal and the amount of torque they provide. The servos can be used as a chain that provide great mobility at every joint.  Behind the robot's head, we can fin the ethernet/internet cable and a memory card slot which can be easily confused as a USB, but it's not. Also, there are differences between micro controllers and full computers. Micro controllers are great at connecting between device, like talking to servos, etc. Full computers, on the other hand, requires an operating system such as Linux. \n",
    "\n",
    "A NAO robot can be programmed using high-level languages such as Python, low-level languages such as C, and a built-in language called Choregraphe. \n",
    "\n",
    "Goals for the summer:\n",
    "- How can we use these robots better? In the lab? In the classroom? At BMC? \n",
    "- We can make them easier for beginner students to learn code, specially students in Cognitive Science\n",
    "- What would a student in Cognitive Science want to learn?\n",
    "- Maybe make the robot move its head, arms, and feet. Maybe we can have a set-up where the robot can play a little/easy game versus another robot or a human. Think about mobility and interaction with others. \n",
    "- Can we create a stand or a little chair/table for the robot?\n",
    "- Ideally, the robot would not be wasting the energy of its legs by standing. It would be nice if it could be hanging or sitting down while it uses its arms to touch things on a table. If it's sitting down it could also touch/press different buttons by using vision recognition and its ultrasonic sensors. We would probably rely on its vision for the most part since the ultrasonic sensors are not that useful in telling us the object position, just the relative distance. \n",
    "- Can we incorporate fingers to the robot?\n",
    "- As my own goal, I would like the robot to grab things around with its hands. In this way, we can have more control/manipulation over what it does and how it interacts. \n",
    "\n",
    "Limitations:\n",
    "- A lot of energy is spent/wasted if the robot is standing. \n",
    "- Fingers are not articulating. The fingers cannot be articulated by using programming. \n",
    "\n",
    "Goals for today:\n",
    "- Get Nao's factory resetted, it's better to connect it directly it with cable rather than wi-fi. Ideally, we would like to control it from factory settings.\n",
    "- Take a look at the Python coding\n",
    "- Get introduced to Summer Science Research coordinators\n",
    "\n",
    "Keep a journal of what you do everyday:\n",
    "- Detailed record of what you have done/tasks you have accomplished\n",
    "- Record of what the professor has helped you do\n",
    "\n",
    "Resetting the robot to its factory settings was not as easy as I expected. In order to do this we needed:\n",
    "- Download NAO Flasher online\n",
    "- Download Naoqi image (from CD/DVD)\n",
    "- Clean/unformatted usb drive\n",
    "- Fully charged robot\n",
    "\n",
    "While trying to reset the NAO robot to its factory settings, we encountered several problems. First, since the robot is around 5 years old, we could not find the Naoqi image we needed because our system is already \"outdated\". We had to use the original CD/DVD included with the robot to find the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 2\n",
    "### 06/07/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While trying to reset the Naos to its factory settings, I encountered a few problems. First, I needed to find an updated firmware of Nao's image but because the robots are not on the market anymore, I could not find the latest image online. Also, after several attempts in trying to clean the USB drive using the Windows Formatting Utility, I finally realized that I needed to use Diskpart to format my drive. \n",
    "\n",
    "In order to use Diskpart and format my USB drive, I performed the following steps:\n",
    "- Open Command Window\n",
    "- Type \"list volume\" and hit Enter\n",
    "- Type \"select volume X\" (where X represents the letter corresponding to my USB removable drive).\n",
    "- Type \"clean\". Remember that this will erase all data and partitions on the drive, and this can't be undone. \n",
    "- Type \"exit\" to exist the Diskpart and the Command Window. \n",
    "\n",
    "After I cleaned my USB drive, I was finally able to open NAO Flasher and write my naoqi image onto the removable drive. After this, I had to plug my drive into the back of Nao's head, plug him to the power outlet, plug in the internet/Ethernet cable, and press his chest button until its light turned blue. Then, I waited for around 30 minutes until the software was installed inside the Nao.\n",
    "\n",
    "Later on, professor Blank told me about a special hub where I could connect the Nao so that it would get a local IP address. For instance, when I powered in the NAO without the internet/Ethernet cable, it gave me back \"169-254-16-208\" as Nao's IP address. In this case, the number 169 means that is connected to a local network, but one different from the school. If the internet/Ethernet cable from the special hub was connected to Nao's head when I turned him on, then I would get an IP address starting with the number 165. \n",
    "\n",
    "I also learned that while inside a Terminal in a Linux machine, I would get a list of available internet networks if I type \"ifconfig\". I could also try to ping the IP address of a robot by using either \"ssh\" or \"telnet\". SSH stands for Secure Socket Shell which lets administrator access a remote computer by using a network protocol. In order to access Nao's on-board computer, I could type \"ssh nao@169.254.16.208\" and connect to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 3\n",
    "### 06/08/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resetting the Nao to its factory settings, we needed him to be able to connect to the RoboCup wireless network. There was a problem with connecting to the network because first, we did not have the password for the Wi-Fi, and second, I could not ping the IP address of the robot. Even when connected to the hub, the IP address was not necessarily working the way I wanted it to work. Prof. Blank made a few calls and was finally able to get the password for the RoboCup network. I read over Nao's documentation and searched for instructions on how to connect the robot over Wi-Fi, but the documentation only applied for connecting the robot over Choreographe. This was useful if I wanted to use Choreographe, but since I wanted to use the terminal and use the Python module, it was not that useful to me. While trying to connect, I learned about connman services, which would give me back a list of available networks related to the IP address of the robot I ping. Also, typing \"ifconfig wlan0\" would turn the wireless card on in case it was off. And, typing \"iwconfig wlan essid NAME key PASSWORD\" and replacing NAME by the wireless network name and replacing PASSWORD by the password would connect me to the Wi-Fi. I attempted this several times, but it did not work. I knew that I should be able to connect directly to RoboCup using the same IP address of the robot because in previous years, the robots were the only ones with permissions to connect to this special network, but everything I kept trying seemed not to work. Although the documentation was somewhat confusing and did not apply to the Naoqi version I had installed in the Nao, I was able to enter the Nao's webpage by entering its IP address in the search engine. After this, I was able to go over to the \"Network\" tab and enter the password for RoboCup, and I was finally able to connect the Nao to the Wi-Fi. \n",
    "\n",
    "Later in the day, prof. Blank and I met to discuss the next steps in doing research. He mentioned that he wanted to have each NAO in a stand where they could not move freely. His idea was that each NAO could be held by their waist and tied to a vertical pole where the pole could move up and down. Apparently, he did not want the Naos to be walking around because they might fall and break. In addition to being really expensive, the NAOs are not being sold any longer and we cannot order any kind of equipment/repair in case of an accident. After giving me extensive details about his idea, I expressed my disagreement with it because I did not want to limit Nao's capabilities. Using Choreographe, I made the robot dance to a built-in Japanese traditional song to show him that we could do so much more with these robots. Thus, I did not necessarily agree with having them not being able to move around. Professor Blank said that even if the robot looked \"cool\" dancing, there was no science behind the movements the robot was making, it was only a matter of calculating the joint angles and repeating the process in a circle. I realized that he had a point, and I did have to agree. After all, we are only interested in the science behind the learning process of a robot. \n",
    "\n",
    "In discussing vision recognition, prof. Blank and I brainstormed ideas about possible games the robot could play. I wanted to make the robot use its hands (if possible) and make him play a game such as checkers, even if that sounded complicated. Other ideas included tic-tac-toe, rock-paper-scissors, or even pushing or pressing different objects down on a horizontal surface. I wanted to explore how the robot recognized different colors and shapes, but professor Blank mentioned that sometimes robots would recognize \"reflection\" better than \"lighting\". Because objects usually emit back some kind of reflection depending on its color, lighting, and the environment they are in, their cameras are better at picking up reflection and recognizing objects based on that. We finally agreed that I could play with NAO's functions both in Choreographe and Python and keep a log of which methods worked and which methods didn't. This way, I learned to make the robot do different activities using the built-in software and Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture: https://athena.brynmawr.edu/jupyter/user/ksosa/files/Summer%20Research_Journals/image1.JPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 4\n",
    "### 06/09/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While trying to connect the Nao to the wireless network, I encountered the options of connecting to a static or a DHCP configuration. In this case, DHCP stands for Dynamic Host Configuration Protocol and it is the best option when it comes to connecting the Naos to Wi-Fi. \n",
    "\n",
    "I also came across several differences in the programming languages, specially between Java and Python. To clarify, I have never written code in Python before and while discussing the differences between the 2 programming languages with the professor, I was able to understand the key differences. Here are some of the key points I was able to draw out from our conversation:\n",
    "\n",
    "Java requires:\n",
    "- Creating classes\n",
    "- Using static void main\n",
    "- Compiling and running code\n",
    "- Lots of parenthesis, curly braces, square brackets, etc.\n",
    "\n",
    "Python:\n",
    "- Indentation matters\n",
    "- Few parentheses\n",
    "- Uses \"def\" to write a new method\n",
    "\n",
    "Although there are major differences in both programming languages, being able to read code in either of them helps understand the other. \n",
    "\n",
    "In order to start coding and testing NAO's capabilities, I needed to download and install Python, Naoqi, and the Python SDK in my laptop. After installing Python, I was able to open a Python idle and start writing code. In order to use the robot, however, I needed to import \"ALProxy\" from the naoqi module. Right before using the Naos, I needed to be able to connect to it by calling its IP address and the port it's connected to. Since each Nao has a name, I am able to call its IP address by using NAME.brynmawr.edu and I can use the default port 9559. For instance, if I want to use Hedwig, I can connect him by typing \"hedwig.brynmawr.edu\". After connecting the Nao, I can start one of its proxies and start calling the methods already defined in each of them. For example, if I want to make my Nao speak I can initialize the proxy \"ALTextToSpeech\" and then call the method say(). My code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from naoqi import ALProxy\n",
    "tts = ALProxy(\"ALTextToSpeech\", \"hedwig.brynmawr.edu\", 9559)\n",
    "tts.say(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, Hedwig says \"Hello, World\" when we run the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 5\n",
    "### 06/14/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After spending some time testing the methods provided in the Naoqi module, I realized that most of the methods listed in the Naoqi documentation are not available in our version of the software. Version 1.12.5 seems to be one of the oldest one, while version 1.14.5 and version 2.1.4.13 have proxies for RedBall and Face tracking. Understanding that object recognition and object tracking are going to be crucial in pursuing the goals of our research is important as the professor and I try to obtain the Naoqi images for the latest versions. \n",
    "\n",
    "Today, I have been searching for the latest Naoqi images online and reading forums and asking for help. However, because the robot itself is outdated, it is extremely difficult to find help online. Professor Blank made some calls to colleagues on Friday, and after checking my e-mail I realized he was able to get both versions of the Naoqi images. I followed the steps for resetting the robots to its factory settings and installed one robot with each of the versions. Then, my goal was to test out the methods for each version and keep a record of which version we wanted to use. The versions differ a lot since some of the proxies are deprecated in the latest versions, and some of the proxies don't even exist in the previous ones. Using the robot IP address, I was able to access the list of modules installed in each robot. Then, using the documentation online, I was able to test out the methods listed for each specific proxy. Doing this allowed me to verify which methods were available in each version and compare and contrast the pros/cons of using any of them. This way, I would figure out which software we wanted to use. My initial reaction was that I would test out both just in case, but I knew that I wanted to Naos with the latest software version because it made more sense and aligned with our summer's plans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 6\n",
    "### 06/15/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the new Naoqi software, I have been able to test out more methods. Writing code is Python is not as easy as I though because the syntax differs greatly from Java and C++. I learned that in order to create a method or function I need to use \"def:\" which stands for define. Also, instead of writing all the code at once I can open up a Python shell and run the code interactively. This way, I can test out more methods one after the other, receiving feedback right away. For instance, if I want the robot to speak, I can just type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hello_world import Robot\n",
    "nao.say(\"Hello, everyone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where hello_world is the name of the .py file and Robot is a class already being defined inside. Such as \"from filename import class\". \n",
    "\n",
    "Today, I also learned how to change my password in a Linux machine. Ian came by and explained that I need do the following:\n",
    "- Open up a terminal (Go to system tools --> MATE terminal)\n",
    "- Type \"passwd\"\n",
    "- Follow the in-screen instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 7\n",
    "### 06/16/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of accomplishing one of the goals of this summer - making it easier for CS students to use the Naos- is to create a library that students can access to. The library would contain all the methods the Nao can use and students can just import them. Although this idea sounds interesting, I don't completely agree with it. First, the built-in language Choreographe already has a library that includes all the methods that can be called using the Naos. Second, the methods differ depending on which Naoqi version we are using. Until now, I have been collecting the methods that do work, but I am not sure writing a library that already exists is a smart idea.\n",
    " \n",
    "Today I discovered that I can have access to the battery level by initializing the \"ALBattery\" API. For instance, when I run the method getBatteryCharge() listed in the Naoqi documentation, it gives me back the level power of the robot. Then I can use this information as a new entry in my library, or as the professor calls it \"dictionary\".\n",
    " \n",
    "Some of the next steps are:\n",
    "- Write a program that records the arm's position. In other words, it gives me back the angles of each joints.\n",
    "\n",
    "Ideally, we want a program that points to a red ball once it detects it. Otherwise, we want the program to be able to say the ball's position once it sees it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 8\n",
    "### 06/17/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, I have been exploring how to retrieve an image from one of the Naos using the Python IDLE. This seems very complicated as I need to export the image and save it or create a new window where I can see it. I know that I need to be able to use the Image or Video proxy in order to retrieve the picture from Nao’s camera. However, I have not been able to access where Nao saves the pictures once he takes them. The documentation online does a good job in explaining the steps in Linux machines, but since I am using a Windows machine the steps are a little different. Using the samples online and my own ideas, I know that my code will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ImageNAO = videoProxy\n",
    "MyDisplayImageFunction(imageNAO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would access the image in the proxy and then display it. However, because I don’t know where the image is being stored, I can’t retrieve it.\n",
    "Besides writing code and testing it, I have been using Choreographe on the side to make the robot speak, walk, stand, sit down, and have access to what Nao sees. This way, I can keep track of what works on Choreographe and does not work on Python. Some of the features in Choreographe such as Face Detection and RedBall Detection are meant to be executed every time the robot recognizes either a red ball or a face. However, in Python I can’t have access to these detection features because they are listed as “events” rather than methods. In order for an event to work, I need to be able to “raise it”, but I have yet to figure out how to do this. Once I figure out how to raise the events listed in the Naoqi module, I can tell the robot to track a face or a red ball directly from Python. This would help me understand how Nao uses its vision to recognize and track objects/faces. So far, I have been able to accomplish face tracking by using Choreographe, but it would be amazing if I could use it using the Python IDLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 9\n",
    "### 07/25/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, Alex and I brainstormed ideas about possible games the NAOs could play. Alex is a Haverford student whose research focuses on how Naos learn, especially how the robots learn to recognize and track a red ball. Using deep learning and Theano, he has written a network where he trains the robots to track an object once the object is in its vision field. Just as he has trained the robots to use vision recognition, I could also train my robots to play a game; such game can be an extension of his project or a completely different one. Here are some of the ideas we came up with:\n",
    "\n",
    "- Naos can imitate human poses (similar to the game \"what Simone says\")\n",
    "- Naos can answer yes/no questions. For example, a human with his hands raised could stand in front of the robot. Then we could ask the NAO \"Are the person's hands above or below his head?\" or \"is the person standing in two legs?\"\n",
    "- Play the game tick-tack-toe\n",
    "- Make the robot tell the position of objects in its field of vision. For instance, we could train him to divide his field of vision in 4 quadrants and he could give us back the quadrant where the object appears.\n",
    "\n",
    "Although these games sound interesting, they involve many concepts which take up time. For instance, in order for the robot to imitate human poses, he needs to be trained to identify what poses look like. Then, he needs to measure the angles of the limbs of the figure in front of him. After that, he needs to know which joints he wants to move in his own body in order to imitate the human pose. Of course, we can make him imitate just the position of a pair of human arms. However, even for such simple result, we need to measure the angles for the ShoulderPitch, ShoudlerRoll ElbowYaw, and ElbowRoll joints.\n",
    "\n",
    "On the other hand, if the robot were to tell the position of an object in its field of vision, we would need to measure the angles for the HeadYaw and the HeadPitch joints. Then, I would need to use Theano and deep learning to train the robot to recognize 4 quadrants. This would be an extended version of Alex's project.\n",
    "\n",
    "Lastly, because our Nao's fingers and hands are not articulating, it is extremely difficult (if not impossible) to play tick-tack-toe with his hands. Another way of making the robot play the game would be to have him use his feet sensors and move around a giant grid in order to push objects slightly with its feet. That could work, but we would be violating our goal of keeping the robots in their respective stand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 10\n",
    "### 07/26/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After considering different games, the robot can play, I have decided for the robot to play tic-tac-toe. This game is very simple for humans to play, but for robots it could be beyond the realms of possibility. However, I think it could be accomplished. Before attempting anything else, I made a list of things to do or things to think about:\n",
    "\n",
    "- Learn to recognize a grid\n",
    "- Recognize two different shapes\n",
    "- Push objects with his feet\n",
    "    - Learn to use feet sensors in Python\n",
    "- Recognize when there is an object already in place\n",
    "- Figure out an algorithm for playing game\n",
    "    - Algorithm could use deep learning \n",
    "    - Robot would be able to learn how to play --> learn from mistakes\n",
    "- Walk around grid and push objects in place\n",
    "- Measure distance from his feet to correct place in grid (how?)\n",
    "\n",
    "The algorithm for tic-tac-toe would include different cases. For instance, we could have if-then statements for each case. Such as:\n",
    "\n",
    "- If robot starts game\n",
    "    - Choose a place in the grid\n",
    "- If another player starts game\n",
    "    - Discard that place and choose a new one\n",
    "- If place X is already taken, discard that place\n",
    "- If two places in a row are taken, choose the remaining place in the row\n",
    "\n",
    "\n",
    "- We could also have a fixed position for the robot to start. For instance, the robot could always start in the middle. \n",
    "\n",
    "Having these cases would ensure that the algorithm for playing the game works. We can add more cases as we think about, and we can translate them into Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 11\n",
    "### 07/27/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several steps that must be followed in order to teach the robot how to play tic-tac-toe. I made a short list that embodies the big ideas:\n",
    "\n",
    "- Use vision recognition \n",
    "- Use feet sensors\n",
    "- Implement tic-tac-toe algorithm\n",
    "\n",
    "These are the main ideas that will drive our research forward. As a first step, I need to learn how the robot uses its camera to identify objects. Some of the questions I would like to explore are the following:\n",
    "\n",
    "- Can the robot identify ANY object? Is it able to track them? \n",
    "- How big should the objects be?\n",
    "- Can we use Alex's program to make the robot search for an object in a single picture and then be able to track it? \n",
    "- How about using its feet? Are we able to make the robot determine the distance it needs to walk?\n",
    "- Can the robot walk a short distance without falling?\n",
    "- Does the environment that the robot is in matters? Would too much light or a light too dim interfere with the robot's vision?\n",
    "\n",
    "These are questions that will take some time to answer. For now, however, the main focus should be in vision recognition. \n",
    "\n",
    "As I was trying to apply the \"ALVisionRecognition\" API today, I noticed that this software is not installed in our version of the robot. I tried to import the module and use it with Python, but since it did not work I opened up Choreographe to see if it worked. Choreographe gave me back feedback saying that the vision recognition module was missing in Naoqi. I accessed Nao's website to verify which Naoqi packages and APIs were installed in version 2.1.3.14 of Naoqi, but as expected I could not find the vision recognition software. The next step would be to downgrade the firmware of our robot, either to version 1.14.5 or even to version 1.12.5. Tomorrow, I will reset the robot called Wol to factory settings and install version 1.14.5. Then, I will verify if this version has the vision recognition API because it is undeniably needed to move forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 12\n",
    "### 07/28/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, I resetted Wol to factory settings using Naoqi version 1.14.5. After checking Nao's website to access Naoqi's packages, I could tell ALVisualToolbox was installed. This module, of course, has the ALVisualRecognition API within it. My next step was to test whether this API worked and whether it recognized any object. Below is what I found:\n",
    "\n",
    "In Choreographe, there is a feature called \"Learn\" which allows the robot to store and recognize an object. In order to use this component, I must be able to activate Nao's video monitor by going to the main menu and clicking on \"View\" and then checking \"Video Monitor\". Now, I will be able to see Nao's camera in the top right corner of my screen. Then, I need to click on \"New Vision Recognition Database\" in order to create a new, clean, database where the robot will store my object. A new window will pop up saying the operation succeeded and I can click \"okay\". I can then notice that the button we just pressed is grayed out and that is what I want. Later, I can hit the universal sign for \"play\" (the triangle - if it's not already pressed) and make sure Nao's camera is streaming live. I can also check whether the camera Nao is using is the camera I want, I can switch Nao's camera by dragging the \"Select Camera\" method. Once I have the correct camera selected, I can place the object in Nao's field of vision. Once the object is in place, I can then click on \"Learn\". A timer appears, counting down from 4 to 1 seconds, and once it ends I can see a still photo of my object. I can click on top of my object and it automatically brings a selection tool. In other words, I need to be able to cut out my object, starting and ending in the same position. Once I finish, a new window pops up asking me to describe the selected area. I can give my object a name and a side, and add a book or location if I want (although it is not necessary), then I can click \"okay\" again. Once the object is stored in the current database, I will get the message:\n",
    "\n",
    "\"Congratulations! The object has been added to your local database. Don't forget to send your local database to the robot\". \n",
    "\n",
    "I click \"okay\" once more, and then I can add as many objects or as many sides of my object as I want. Once I have finished adding all my object to the local database, I can click on \"Send current recognition database to the robot\" and then a new window will appear warning me that the current local database will be replaced, and if I am sure I can just click \"yes\". Once I do this, I can test out whether my Nao recognizes the objects I just saved. I can also save a copy of the database by clicking on \"Export database\", and selecting the folder where I want to save my new object recognition database. \n",
    "\n",
    "Because I am able to use a selection tool, I can save any object using my vision recognition tool in Choreographe. For instance, I saved both my cross and my circle in a vision recognition database. Then, I had the robot say their names every time he saw them. The API worked perfectly fine, so now I will need to figure out whether I can do the same using my Python Idle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.youtube.com/watch?v=BigrGEvFTJs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 13\n",
    "### 07/29/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I know the vision recognition software can store and recognize any object in a new database using Choreographe. Using the  vision recognition toolbox in Python, however, is not as easy. Today, I tried to use the ALVisionRecognition API in Python, but I encountered a few limitations:\n",
    "\n",
    "- Video monitor needs to be accessed to get live Nao images\n",
    "- The feature \"Learn\" is not a method or an event that can be called\n",
    "- I don't have access to creating, sending over, importing, or exporting a new vision recognition database\n",
    "- I need to be able to use a selection toolbox to cut out my object\n",
    "- Video monitor needs to be accessed to get live Nao images\n",
    "\n",
    "As a first step, I thought it would be awesome to have access to Nao's camera. Taking a picture and retrieving it is already complicated, let alone retrieving video in real-time. However, after looking over the ALVideoDevice API I found out some samples in Python that might help me. Here is the sample code I found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a tiny example that shows how to show live images from Nao using PyQt.\n",
    "# You must have python-qt4 installed on your system.\n",
    "#\n",
    "\n",
    "import sys\n",
    "\n",
    "from PyQt4.QtGui import QWidget, QImage, QApplication, QPainter\n",
    "from naoqi import ALProxy\n",
    "\n",
    "# To get the constants relative to the video.\n",
    "import vision_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageWidget(QWidget):\n",
    "    \"\"\"\n",
    "    Tiny widget to display camera images from Naoqi.\n",
    "    \"\"\"\n",
    "    def __init__(self, IP, PORT, CameraID, parent=None):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        QWidget.__init__(self, parent)\n",
    "        self._image = QImage()\n",
    "        self.setWindowTitle('Nao')\n",
    "\n",
    "        self._imgWidth = 320\n",
    "        self._imgHeight = 240\n",
    "        self._cameraID = CameraID\n",
    "        self.resize(self._imgWidth, self._imgHeight)\n",
    "\n",
    "        # Proxy to ALVideoDevice.\n",
    "        self._videoProxy = None\n",
    "\n",
    "        # Our video module name.\n",
    "        self._imgClient = \"\"\n",
    "\n",
    "        # This will contain this alImage we get from Nao.\n",
    "        self._alImage = None\n",
    "\n",
    "        self._registerImageClient(IP, PORT)\n",
    "\n",
    "        # Trigget 'timerEvent' every 100 ms.\n",
    "        self.startTimer(100)\n",
    "\n",
    "\n",
    "    def _registerImageClient(self, IP, PORT):\n",
    "        \"\"\"\n",
    "        Register our video module to the robot.\n",
    "        \"\"\"\n",
    "        self._videoProxy = ALProxy(\"ALVideoDevice\", IP, PORT)\n",
    "        resolution = vision_definitions.kQVGA  # 320 * 240\n",
    "        colorSpace = vision_definitions.kRGBColorSpace\n",
    "        self._imgClient = self._videoProxy.subscribe(\"_client\", resolution, colorSpace, 5)\n",
    "\n",
    "        # Select camera.\n",
    "        self._videoProxy.setParam(vision_definitions.kCameraSelectID,\n",
    "                                  self._cameraID)\n",
    "\n",
    "\n",
    "    def _unregisterImageClient(self):\n",
    "        \"\"\"\n",
    "        Unregister our naoqi video module.\n",
    "        \"\"\"\n",
    "        if self._imgClient != \"\":\n",
    "            self._videoProxy.unsubscribe(self._imgClient)\n",
    "\n",
    "\n",
    "    def paintEvent(self, event):\n",
    "        \"\"\"\n",
    "        Draw the QImage on screen.\n",
    "        \"\"\"\n",
    "        painter = QPainter(self)\n",
    "        painter.drawImage(painter.viewport(), self._image)\n",
    "\n",
    "\n",
    "    def _updateImage(self):\n",
    "        \"\"\"\n",
    "        Retrieve a new image from Nao.\n",
    "        \"\"\"\n",
    "        self._alImage = self._videoProxy.getImageRemote(self._imgClient)\n",
    "        self._image = QImage(self._alImage[6],           # Pixel array.\n",
    "                             self._alImage[0],           # Width.\n",
    "                             self._alImage[1],           # Height.\n",
    "                             QImage.Format_RGB888)\n",
    "\n",
    "\n",
    "    def timerEvent(self, event):\n",
    "        \"\"\"\n",
    "        Called periodically. Retrieve a nao image, and update the widget.\n",
    "        \"\"\"\n",
    "        self._updateImage()\n",
    "        self.update()\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        When the widget is deleted, we unregister our naoqi video module.\n",
    "        \"\"\"\n",
    "        self._unregisterImageClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    IP = \"nao.local\"  # Replace here with your NaoQi's IP address.\n",
    "    PORT = 9559\n",
    "    CameraID = 0\n",
    "\n",
    "    # Read IP address from first argument if any.\n",
    "    if len(sys.argv) > 1:\n",
    "        IP = sys.argv[1]\n",
    "\n",
    "    # Read CameraID from second argument if any.\n",
    "    if len(sys.argv) > 2:\n",
    "        CameraID = int(sys.argv[2])\n",
    "\n",
    "\n",
    "    app = QApplication(sys.argv)\n",
    "    myWidget = ImageWidget(IP, PORT, CameraID)\n",
    "    myWidget.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing the code, I had to install the PyQt4 packet in Python, but once installed the program would run nicely. For instance, when I let the code run in the Python Idle it would create a new window called \"Nao\" that gives me stream live video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://doc.aldebaran.com/1-14/dev/python/examples/vision/get_image.html#python-example-vision-getimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 14\n",
    "### 08/01/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have access to Nao’s live images, I can try to call the vision recognition software.  This is very difficult as I do not have an option to import a selection tool along with the video image. I searched throughout the documentation to see if there was a way of using the “Learn” button in Python, but I could not find anything useful. Using the Learn feature in the same way that it is used in Choreographe would give me access to creating a new vision recognition database, but I have had no luck so far.\n",
    " \n",
    "After realizing that using the ALVisionRecognition API might be harder to use than expected, I started searching for ways where the robot would perform an action (such as say hello, or raise his hand) when it encountered an object. I discovered that Naoqi sometimes would raise “Events” as opposed to calling methods. In order to use an event listed in the Naoqi documentation I must be able to subscribe and unsubscribe to it. For instance, if I want to use the event called “FaceDetected” I need to subscribe to it. In order to call the Subscribe() function in the ALMemory Module, I need the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "void ALMemoryProxy::subscribeToEvent(const std::string& name, const std::string& callbackModule, const std::string& callbackMethod)\n",
    "\n",
    "Subscribes to an event and automatically launches the module that declared itself as the generator of the event if required.\n",
    "\n",
    "Parameters:\n",
    "- name – The name of the event to subscribe to\n",
    "- callbackModule – Name of the module to call with notifications\n",
    "- callbackMethod – Name of the module’s method to call when a data is changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I can also unsubscribe to an event by calling the Unsubscribe() function in the ALMemory Module, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "void ALMemoryProxy::unsubscribeToEvent(const std::string& name, const std::string& callbackModule)\n",
    "\n",
    "Unsubscribes a module from the given event. No further notifications will be received. The module capable of generating the event is automatically stopped when the last subscriber unsubscribes.\n",
    "\n",
    "Parameters:\n",
    "- name – The name of the event\n",
    "- callbackModule – The name of the module that was given when subscribing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://doc.aldebaran.com/2-1/naoqi/core/almemory-api.html#almemory-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 15\n",
    "### 08/02/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing how to subscribe and to unsubscribe to events is  important because I can be able to use “FaceDetected” and “RedBallDetected” using my ALVisionRecognition API. In other words, I could have a program such that every time a face or a ball is detected, the robot could say it recognizes it. When thinking about the game, this would be helpful as we would know when the robot sees the cross or sees the circle. While searching the documentation online, I found a sample program where the robot says “Hey you!” every time it sees a face. In our case, we need to think about the steps the robot needs to make in order for the tic-tac-toe game to be successful. Here are some of steps I have thought about so far:\n",
    "\n",
    "- See object (either cross or circle) and recognize it\n",
    "- Track the object with head\n",
    "- If not in “stand up” position, then stand up\n",
    "- Walk towards object\n",
    "- Move the object with its feet to the desired position\n",
    "\n",
    "Potentially, I could tweak the sample code so that instead of subscribing to the FaceDetected event, it could subscribe to the RedBallDetected and say “object detected” when it encounters the red ball. \n",
    "\n",
    "Below is the sample code  I found online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Say 'hello, you' each time a human face is detected\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from naoqi import ALProxy\n",
    "from naoqi import ALBroker\n",
    "from naoqi import ALModule\n",
    "\n",
    "from optparse import OptionParser\n",
    "\n",
    "NAO_IP = \"nao.local\"\n",
    "\n",
    "\n",
    "# Global variable to store the HumanGreeter module instance\n",
    "HumanGreeter = None\n",
    "memory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HumanGreeterModule(ALModule):\n",
    "    \"\"\" A simple module able to react\n",
    "    to facedetection events\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        ALModule.__init__(self, name)\n",
    "        # No need for IP and port here because\n",
    "        # we have our Python broker connected to NAOqi broker\n",
    "\n",
    "        # Create a proxy to ALTextToSpeech for later use\n",
    "        self.tts = ALProxy(\"ALTextToSpeech\")\n",
    "\n",
    "        # Subscribe to the FaceDetected event:\n",
    "        global memory\n",
    "        memory = ALProxy(\"ALMemory\")\n",
    "        memory.subscribeToEvent(\"FaceDetected\",\n",
    "            \"HumanGreeter\",\n",
    "            \"onFaceDetected\")\n",
    "\n",
    "    def onFaceDetected(self, *_args):\n",
    "        \"\"\" This will be called each time a face is\n",
    "        detected.\n",
    "\n",
    "        \"\"\"\n",
    "        # Unsubscribe to the event when talking,\n",
    "        # to avoid repetitions\n",
    "        memory.unsubscribeToEvent(\"FaceDetected\",\n",
    "            \"HumanGreeter\")\n",
    "\n",
    "        self.tts.say(\"Hello, you\")\n",
    "\n",
    "        # Subscribe again to the event\n",
    "        memory.subscribeToEvent(\"FaceDetected\",\n",
    "            \"HumanGreeter\",\n",
    "            \"onFaceDetected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" Main entry point\n",
    "\n",
    "    \"\"\"\n",
    "    parser = OptionParser()\n",
    "    parser.add_option(\"--pip\",\n",
    "        help=\"Parent broker port. The IP address or your robot\",\n",
    "        dest=\"pip\")\n",
    "    parser.add_option(\"--pport\",\n",
    "        help=\"Parent broker port. The port NAOqi is listening to\",\n",
    "        dest=\"pport\",\n",
    "        type=\"int\")\n",
    "    parser.set_defaults(\n",
    "        pip=NAO_IP,\n",
    "        pport=9559)\n",
    "\n",
    "    (opts, args_) = parser.parse_args()\n",
    "    pip   = opts.pip\n",
    "    pport = opts.pport\n",
    "\n",
    "    # We need this broker to be able to construct\n",
    "    # NAOqi modules and subscribe to other modules\n",
    "    # The broker must stay alive until the program exists\n",
    "    myBroker = ALBroker(\"myBroker\",\n",
    "       \"0.0.0.0\",   # listen to anyone\n",
    "       0,           # find a free port and use it\n",
    "       pip,         # parent broker IP\n",
    "       pport)       # parent broker port\n",
    "\n",
    "\n",
    "    # Warning: HumanGreeter must be a global variable\n",
    "    # The name given to the constructor must be the name of the\n",
    "    # variable\n",
    "    global HumanGreeter\n",
    "    HumanGreeter = HumanGreeterModule(\"HumanGreeter\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print\n",
    "        print \"Interrupted by user, shutting down\"\n",
    "        myBroker.shutdown()\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://doc.aldebaran.com/1-14/dev/python/reacting_to_events.html\n",
    "http://doc.aldebaran.com/1-14/dev/python/reacting_to_events.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 16\n",
    "### 08/03/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying to use the online sample I found, I realized that there is still no way to add any object into my vision recognition software. I can change the FaceDetected event to be RedBallDetected so that the robot says \"red ball detected\" each time it sees the ball. When trying to add any other object, however, the vision recognition software does not work. As a next step, I tweaked Alex's code hoping that the robot could take a picture of the object, save it, and then search for the object in that picture. In other words, I could tell the robot to recognize and potentially track the object within the image. For instance, I could still use Alex's method called save_photo in order to take and save the photo. Then, I created a method called recognize_and_track where the robot would access the saved picture and recognize the object within it, and once it recognized it, it would say \"found object_name\". The code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from naoqi import ALProxy\n",
    "import time\n",
    "import sys\n",
    "import Image\n",
    "import vision_definitions\n",
    "\n",
    "#IP's of NAOs\n",
    "    #hedwig - 165.106.241.201\n",
    "    #hoots - 165.106.241.202\n",
    "    #nyctimene - 165.106.241.203\n",
    "    #wol - 165.106.241.204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Robot:\n",
    "    def __init__(self, ip, port):\n",
    "        #stuff I'll need to initialize here:\n",
    "        self.ip = ip\n",
    "        self.port = port\n",
    "        self.tts = ALProxy(\"ALTextToSpeech\", ip, port)\n",
    "        self.rbd = ALProxy(\"ALRedBallTracker\", ip, port)\n",
    "        self.visual = ALProxy(\"ALVideoDevice\", ip, port)\n",
    "        self.motion = ALProxy(\"ALMotion\", ip, port)\n",
    "        self.memory = ALProxy(\"ALMemory\", ip, port)\n",
    "        self.life = ALProxy(\"ALAutonomousLife\", ip, port)\n",
    "        self.posture = ALProxy(\"ALRobotPosture\", ip, port)\n",
    "        self.aware = ALProxy(\"ALBasicAwareness\", ip, port)\n",
    "        self.behave = ALProxy(\"ALBehaviorManager\", ip, port)\n",
    "        self.tracker = ALProxy(\"ALTracker\", ip, port)\n",
    "        self.tts.say(\"ok\")\n",
    "        self.life.setState(\"disabled\")\n",
    "\n",
    "    def set_neutral(self):\n",
    "        self.motion.setStiffnesses(\"Head\", 1.0)\n",
    "        self.motion.setAngles(\"HeadPitch\", 0, 0.1)\n",
    "        self.motion.setAngles(\"HeadYaw\", 0, 0.1)\n",
    "        time.sleep(3.0)\n",
    "        self.motion.setStiffnesses(\"Head\", 0)\n",
    "        \n",
    "\n",
    "    def save_photo(self, img_name): #takes 30x40 photo and saves to .py file directory\n",
    "        print (\"taking photo: \" + img_name)\n",
    "        vidClient = self.visual.subscribeCamera(\"python_client\", 0, 2, 11, 5) #initialize cam with settings\n",
    "        naoImage = self.visual.getImageRemote(vidClient) #taking photo\n",
    "        time.sleep(0.5)\n",
    "        self.visual.unsubscribe(vidClient) #un-initialize\n",
    "        imageWidth = naoImage[0]\n",
    "        imageHeight = naoImage[1]\n",
    "        array = naoImage[6]\n",
    "        img = Image.frombytes(\"RGB\", (imageWidth, imageHeight), array)\n",
    "        img.save(img_name + \".jpeg\", \"JPEG\")\n",
    "        print (\"photo \" + img_name + \" saved!\")\n",
    "\n",
    "    def recognize_n_track(self, target_name, img_name):\n",
    "        #sets nao's head to neutral, saves an image, then tracks the object\n",
    "        #says its found it, return the joint angles of its head, and stops tracking\n",
    "        \n",
    "        self.set_neutral()\n",
    "        time.sleep(1.0)\n",
    "        self.save_photo(img_name)\n",
    "        self.motion.setStiffnesses(\"Head\", 1.0)\n",
    "        self.rbd.startTracker()\n",
    "        print \"starting tracker\"\n",
    "        time.sleep(1.5)\n",
    "        self.tts.say(\"found \" + target_name)\n",
    "        temp3 = str(self.motion.getAngles(\"HeadYaw\", True))[1:-1]\n",
    "        temp4 = str(self.motion.getAngles(\"HeadPitch\", True))[1:-1]\n",
    "        temp5 = target_name\n",
    "        fp = open(\"naoheadangles.txt\", \"a\")\n",
    "        fp.write(temp3 + \", \" + temp4 + \", \" + temp5 + \"\\n\") #writing angles into .txt file\n",
    "        fp.close()\n",
    "        time.sleep(15.0)\n",
    "        print \"stopping tracker\"\n",
    "        self.rbd.stopTracker()\n",
    "        self.motion.setStiffnesses(\"Head\", 0)\n",
    "        self.set_neutral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    hedwig = Robot(\"hedwig.brynmawr.edu\", 9559)\n",
    "    hedwig.save_photo(\"crossPhoto1\")\n",
    "    hedwig.recognize_n_track(\"cross\", \"crossPhoto1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executed, the code would print out the following:\n",
    "\n",
    "https://athena.brynmawr.edu/jupyter/user/ksosa/files/Summer%20Research_Journals/feedback.png\n",
    "\n",
    "And here is the picture it took:\n",
    "\n",
    "https://athena.brynmawr.edu/jupyter/user/ksosa/files/Summer%20Research_Journals/crossPhoto1.jpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Keisna Sosa\n",
    "### Journal 17\n",
    "### 08/04/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the robot recognize an object from a picture works fine, however, there is one detail that still needs to be addressed. In the method called recognize_and_track, I used the \"ALRedBallTracker\" API in order to track both my cross and my circle, but ideally, I would like to use the \"ALTracker\" API instead. By using the ALTracker module I have access to other methods such as lookAt(), pointAt(), getTargetPosition(), and getRobotPosition() which I might use later. So far my code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.rbd = ALProxy(\"ALRedBallTracker\", ip, port)\n",
    "self.rbd.startTracker()\n",
    "self.rbd.stopTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, I would like my code to look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.tracker = ALProxy(\"ALTracker\", ip, port)\n",
    "self.tracker.registerTarget(\"cross\", 0.06)\n",
    "self.tracker.track(\"cross\")\n",
    "self.tracker.stopTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I would add an object called \"cross\" and then track it. I would also be able to call other methods and have the robot point at the object with one or both arms. While attempting to test the ALTracker module, I realized that such module does not exist in version 1.14.5 of our robot, but it does exist in the later version 2.1.3.14. Thus, my next step would be to make a decision on which module is more important: Using the ALVisionRecognition software or the ALTracker? Both APIs are essential in our research and thus, both are needed in order to complete the game.\n",
    "\n",
    "After discussing the pro's and con's of each API with professor Blank, I decided that using the ALTracker software might be more helpful. For instance, if the system is capable of tracking an object it must also be able to recognize it. In other words, the vision recognition software might already be included in the module. In terms of using the Naos in future research and in the classroom, it is also important for them to be updated using the latest firmware. Thus, my next step was resetting all the Nao's to the 2.1.3.14 Naoqi image again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 18\n",
    "### 08/05/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, ideally I would like to use my ALTracker API in order to add new objects and be able to track them. Currently, the Naos are using Naoqi version 2.1.4.13  which includes the ALTracker API, but not the ALVisionRecognition API. I created a new method called add_target to test out whether registerTarget() works. The idea is that I need to register a target, get a list of registered targets to double-check my object was added, and then be able to start and stop the tracker. After attempting to add “circle” and “cross” several times, I would call the methods isNewTargetDetected() and getRegisteredTargets(), but I would get back an empty square bracket which meant that the new object was not being added. I tweaked the code a bit and I tried  to run it several more times, but I kept encountering the same problem. Then, I talked to professor Blank about the problems I was encountering and he suggested to test the code interactively instead. He showed me that In order to do this, I had to make my Robot class  a global variable, and then create a new robot in another jupyter cell. Then, I could execute each line of code one after another, getting feedback immediately. He tried to add “cross” and “circle” as new target names, but the objects would not be added. Then, after checking the Naoqi documentation website, we discovered that there are limited targets the Nao can add into its database. In other words, Nao is only allowed to register and track  “RedBall”, “Face”, “LandMark”,  “LandMarks”, “People” and “Sound”. When trying to register any of the targets allowed, the program would successfully register them. \n",
    "\n",
    "After discussing the limitations I had encountered thus far with Prof. Blank, he said that it might be better to change the trajectory of the research. Since his motto is “don’t get stuck”, we could create a new game similar to peek-a-boo. For instance, the robot would say “I see the ball” every time it recognized it, and it would say “I don’t see the ball” when the ball was out of reach. Since I had worked on a similar example before, I pulled out the code and tweaked it. This time the robot did exactly what I wanted him to do when he recognized the red ball. However, this game still seemed fairly easy and a little different than the actual peek-a-boo game. In order to make the robot resemble the game a bit more, I decided to add the actual arm movements. For instance the robot could cover his eyes for a few seconds, and then uncover them for a few more seconds and keep repeating the same movement in a loop. I created a method called get_angles that would give me back the ShoulderPitch, ShoulderRoll, ElbowYaw, and ElbowRoll angles. This method would first tell the robot to sit down, write the angles for the sitting position in a text file, then give me 10 seconds to raise the robot arms up, and then write the angles in the text file for the raised arms position as well. This is how I was able to gather the angles for each joint, and then I could use this information to write the methods cover_eyes and uncover_eyes. Knowing the angles allowed me to set each joint into the correct position so that the robot could cover and uncover his eyes when the methods were called. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://doc.aldebaran.com/2-1/naoqi/trackers/altracker.html and http://doc.aldebaran.com/1-14/naoqi/motion/control-joint.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 19\n",
    "### 08/09/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the weekend, professor Blank brought some rugs to protect the robots in case they fall. After testing several postures, I noticed that the Naos are even more likely to fall in the rug. One theory is that the friction in the mat is greater than the friction of the floor and so the robot has trouble moving around. Also, the color and texture of the mat might be confusing Nao’s vision because the robot slips a little further when it tries to sit down, and therefore he falls.  It is still important to protect the Naos from falling down and breaking, thus, I will discuss other ideas with prof. Blank once he is back in the lab. \n",
    "\n",
    "Also, I have been working in making the robot play the game \"Looking for the red ball\". I wanted the Nao to cover his eyes and say \"I don't see the red ball\" for a few seconds, before he moves back to his original sitting position, uncovers his eyes, and starts recognizing the ball once he sees it. He does a great job in raising the event called RedBallDetected and saying \"I see the red ball\" once the ball is in front of his camera. Since I wanted him to cover and uncover his eyes every few seconds I noticed that there are several ways of doing this:\n",
    "\n",
    "- Using the idea of \"state\"\n",
    "- Using the idea of \"time\"\n",
    "- Using some kind of loop, in this case a while loop can be used with time()\n",
    "\n",
    "I figured that the best way to approach the problem would be to use a while loop and the idea of time. For instance, I can have a while loop where the robot covers his eyes for 10 seconds and says he does not see the ball. Then, once it exits out of the loop, I can have the robot uncover his eyes and subscribe to the event called RedBallDetected. The only problem which I still need to figure out would be how to keep running both events, one after the other, several times in a row. In other words, I want the robot to keep covering and uncovering his eyes, say every 10 seconds, until the program exits out. What would be a good way to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keisna Sosa\n",
    "### Journal 20\n",
    "### 08/10/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, I have been searching for different ways in which to make my robot cover his eyes and uncover them every 10 seconds. I was using a while loop and the idea of time to make the robot raise his hands and cover his eyes for 10 seconds. However, once it exits out the while loop, it would run the code for uncover_eyes and would not cover his eyes again. I was introduced to threading by my lab partner, and after Googling what threading meant and how it worked; I tried to write some code on my own. First, I noticed that I could not use the idea of threading yet because the code I had was running line after line inside the constructor. In other words, I did not have a method that would call the code directly. I thought that the best way to approach the problem was to put all my code in different methods and I created two methods called cover_eyes() and uncover_eyes(). This took me a while as I was defining the methods outside my main constructor _init_() and calling them inside. As expected, this produced an error. After moving them inside the constructor and placing them right before my call, I was able to do the right assignment. Then, I imported threading and wrote the following lines of code:\n",
    "\n",
    "    t = threading. Thread (10.0, cover_eyes, [self])\n",
    "    t.start\n",
    "\n",
    "To me, this seemed correct, but it did not work. I tried different ways of implementing threading into my code, but nothing seemed to work. My next step would be to watch tutorials on how to write my own function using threading that runs every 10 second. Otherwise, I could use the idea of \"state\" in order for the robot to run temporarily on the cover_eyes state and after a few seconds, it can switch to the uncover_eyes state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
